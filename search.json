[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nexpected value\nvariance\nstandard deviation\n\n\n\nlibrary(dplyr)\n#Getting Ages and their Prob\n\nrandom_vars &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/random_vars.rds\")\n\nunique_ages&lt;-unique(random_vars[\"age\"]) %&gt;% as.matrix()\nordered_ages&lt;-unique_ages[order(unique_ages, decreasing = FALSE)]\nprob_ages&lt;-prop.table(table(random_vars$age)) %&gt;% unclass()\n\n#Getting Income and their Prob\nunique_inc&lt;-unique(random_vars[\"income\"]) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(random_vars$income)) %&gt;% unclass()\n\nE_ages&lt;-sum(ordered_ages*prob_ages)\nE_inc&lt;-sum(ordered_inc*prob_inc)\n\n#Getting Variance\nVar_ages&lt;-var(random_vars$age)\nVar_inc&lt;-var(random_vars$income)\n\nSd_ages&lt;-sd(random_vars$age)\nSd_inc&lt;-sd(random_vars$income)\n\n\n\n#&gt; [1] \"E(Ages): 33.471\"\n\n\n#&gt; [1] \"E(Income): 3510.731\"\n\n\n#&gt; [1] \"Var(Ages): 340.607766766767\"\n\n\n#&gt; [1] \"Var(Income): 8625645.84448348\"\n\n\n#&gt; [1] \"SD(Ages): 18.4555619466536\"\n\n\n#&gt; [1] \"SD(Income): 2936.94498492626\"\n\n\n\nExplain, if it makes sense to compare the standard deviations.\n\nIt doesn’t make sense to compare the standard deviation. Both have different ranges of values in the population therefor comparisons of just the SD gives us no meaningful information.\n\nThen, examine the relationship between both variables and compute:\n\ncovariance\ncorrelation\n\n\n\nlibrary(dplyr)\n\ncov_data&lt;-cov(random_vars$age,random_vars$income)\ncor_data&lt;-cor(random_vars$age,random_vars$income)\n\n\n\n#&gt; [1] \"Covariance: 29700.1468458458\"\n\n\n#&gt; [1] \"Correlation: 0.547943162326477\"\n\n\n\nWhat measure is easier to interpret? Please discuss your interpretation.\n\nCorrelation is easier to interpret. Due to the normalization of correlation between -1 to 1 on can precisely determine how strong/weak the relation where has due to the boundless nature of covariance one can only say that the relation between the variables is strong/weak but not its extent.\n\nlibrary(dplyr)\n#Set 1\nset1&lt;-random_vars$income[random_vars$age&lt;=18]\n\nunique_inc&lt;-unique(set1) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set1)) %&gt;% unclass()\n\nE_set1&lt;-sum(ordered_inc*prob_inc)\n\n#Set 2\nset2&lt;-random_vars$income[random_vars$age&gt;=18 & random_vars$age&lt;65]\n\nunique_inc&lt;-unique(set2) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set2)) %&gt;% unclass()\n\nE_set2&lt;-sum(ordered_inc*prob_inc)\n\n#Set3\nset3&lt;-random_vars$income[random_vars$age&gt;=65]\n\nunique_inc&lt;-unique(set3) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set3)) %&gt;% unclass()\n\nE_set3&lt;-sum(ordered_inc*prob_inc)\n\n\n\n#&gt; [1] \"E[income|age &lt;= 18]: 389.607438016529\"\n\n\n#&gt; [1] \"E[income|age ∈ [18,65)]: 4685.73426573427\"\n\n\n#&gt; [1] \"E[income|age &gt;= 65]: 1777.23728813559\""
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nThink about example from previous chapter (parking spots) and draw the DAG.\n\nWe take the variables as follows * X is the Parking spots * Y is the Sales * Z is the Location\nWe make the folloing assumption for the dependancies, * X and Z: Dependant, Location affects the size of the parking spots * Z and Y: Dependant, Location affects the Sales * X and Y, conditional on Z: Dependant as the size of the parking lot affects sales in a fixed location\n\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n# Chain\nchain &lt;- dagify(\n    X ~ Z,\n    Y ~ Z,\n    Y ~ X,\n    coords = list(x = c(Y = 3, Z = 2, X = 1),\n                  y = c(Y = 0, Z = 1, X = 0)),\n    labels = list(X = \"Parking Spots\",\n                Y = \"Sales\", \n                Z = \"Location\")\n)\n\n# Plot DAG\nggdag(chain) +\n    geom_dag_point(color = \"red\") +\n    geom_dag_text(color = \"white\") +\n    geom_dag_edges(edge_color = \"black\")+\n    geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\nIn the data, you find three variables: satisfaction, follow_ups and subscription. Perform the following steps:\n\nregress satisfaction on follow_ups\nregress satisfaction on follow_ups and account for subscription\n\n\n\ndf &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/customer_sat.rds\")\n\n# Not Conditioned\nlm_1&lt;-lm(satisfaction ~ follow_ups,data=df)\n\n#Plot\nlm_not_cond &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction ))+\n  geom_point()+\n  stat_smooth(method = \"lm\", se = F)\n\n\n#Conditioned\nlm_2&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Elite\", ])\nlm_3&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Premium\", ])\nlm_4&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Premium+\", ])\n#Plot\nlm_cond &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction,color= subscription ))+ geom_point()+stat_smooth(method = \"lm\", se = F)\n\n#Printing\nprint(\"Not conditioned coefficients:\")\n\n#&gt; [1] \"Not conditioned coefficients:\"\n\nlm_1$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   78.886047   -3.309302\n\nprint(\"Conditioned coefficients accounting for Elite:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Elite:\"\n\nlm_2$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   31.307692    1.653846\n\nprint(\"Conditioned coefficients accounting for Premium:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Premium:\"\n\nlm_3$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   70.076923    3.076923\n\nprint(\"Conditioned coefficients accounting for Premium+:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Premium+:\"\n\nlm_4$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;       47.95        1.75\n\nlm_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlm_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assuming the same data as in the Probability tree, the related probabilities and their sum is calculate using the code below.\n\n#Constants Definition N&lt;-Not, I&lt;-|. so P(B|A) = P_BIA\nP_S&lt;-0.3\nP_NS&lt;-1-P_S\nP_TIS&lt;-0.2\nP_NTIS&lt;-1-P_TIS\nP_TINS&lt;-0.6\nP_NTINS&lt;-1-P_TINS\n\n#Calculation\nP1&lt;-P_S*P_TIS\nP2&lt;-P_S*P_NTIS\nP3&lt;-P_NS*P_TINS\nP4&lt;-P_NS*P_NTINS\nSum_of_P&lt;-P1+P2+P3+P4\n\n\n\n\nP(T ⋂ S ) = 0.06\nP(T ⋂ !S ) = 0.24\nP(!T ⋂ S ) = 0.42\nP(!T ⋂ !S ) = 0.28\nSum of all = 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nRead the data and check the dimensions. How many rows and how many columns does the data have?\n\n181 rows and 82 columns\n\nUse appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\nstr() gives us a quick overview of the different datatypes. typeof() gives us a better view of the type of integer data present.\n\ndf &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/car_prices.rds\")\n\nstr(df)\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\ntypeof(df$wheelbase)\n\n#&gt; [1] \"double\"\n\n\n\nRun a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\nRelevant Factors based on p-value: * carbody * carwidth * enginelocation * stroke\n* peakrpm * enginesize\nThese factors were chosen based on their p-values indicating that the alternative hypothesis was true instead of the null hypothesis stating that there is a correlation between the variables in question.\n\nlm_imp &lt;- lm(price ~ carbody+carwidth+enginelocation+stroke+peakrpm+enginesize, data = df)\nsummary(lm_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ carbody + carwidth + enginelocation + stroke + \n#&gt;     peakrpm + enginesize, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8973.1 -1644.5  -136.5  1265.1 15085.1 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.697e+04  1.035e+04  -6.472 9.85e-10 ***\n#&gt; carbodyhardtop     -4.116e+03  1.624e+03  -2.534 0.012181 *  \n#&gt; carbodyhatchback   -3.659e+03  1.295e+03  -2.826 0.005269 ** \n#&gt; carbodysedan       -2.464e+03  1.278e+03  -1.928 0.055483 .  \n#&gt; carbodywagon       -3.624e+03  1.389e+03  -2.609 0.009888 ** \n#&gt; carwidth            9.752e+02  1.661e+02   5.872 2.19e-08 ***\n#&gt; enginelocationrear  1.124e+04  2.112e+03   5.321 3.20e-07 ***\n#&gt; stroke             -2.422e+03  7.119e+02  -3.402 0.000833 ***\n#&gt; peakrpm             1.898e+00  5.516e-01   3.442 0.000726 ***\n#&gt; enginesize          1.311e+02  8.936e+00  14.676  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2864 on 171 degrees of freedom\n#&gt; Multiple R-squared:  0.8803, Adjusted R-squared:  0.874 \n#&gt; F-statistic: 139.7 on 9 and 171 DF,  p-value: &lt; 2.2e-16\n\nconfint(lm_imp, level = 0.95)\n\n#&gt;                            2.5 %        97.5 %\n#&gt; (Intercept)        -8.740074e+04 -46545.470512\n#&gt; carbodyhardtop     -7.321743e+03   -909.386711\n#&gt; carbodyhatchback   -6.215052e+03  -1103.700157\n#&gt; carbodysedan       -4.985517e+03     58.389405\n#&gt; carbodywagon       -6.366137e+03   -882.064242\n#&gt; carwidth            6.473973e+02   1303.044319\n#&gt; enginelocationrear  7.068573e+03  15406.356002\n#&gt; stroke             -3.827414e+03  -1016.802715\n#&gt; peakrpm             8.096928e-01      2.987167\n#&gt; enginesize          1.135026e+02    148.779568\n\n\n\nChoose one regressor and\n\nexplain what data type it is and what values it can take on\nwhat effect is has on the price and what changing the value would have as a result\nwhether its effect is statistically significant.\n\n\nWe can choose to look at the factor “Peak RPM”. This factor has integer values ranging from 4200 to 6600. We see that it has a positive effect on the price of the car. As the “Peak RPM” increases the price increases and indicated by the positive coefficient in the linear function. From the p-value 0.000726 we can infer that this factor is highly statistically significant for the inference of the car prices.\n\nAdd a variable seat_heating to the data and assign a value TRUE for all observations. You can use e.g. df %&gt;% mutate(new_variable = value). Assign it to a new object and run a regression. What coefficient do you get for the new variable seat_heating and how can you explain it?\n\n\nlibrary(dplyr)\n\n\ndf&lt;- df %&gt;% mutate(seat_heating = TRUE)\nlm_imp &lt;- lm(price ~ carbody+carwidth+enginelocation+stroke+peakrpm+enginesize+seat_heating, data = df)\nsummary(lm_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ carbody + carwidth + enginelocation + stroke + \n#&gt;     peakrpm + enginesize + seat_heating, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8973.1 -1644.5  -136.5  1265.1 15085.1 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.697e+04  1.035e+04  -6.472 9.85e-10 ***\n#&gt; carbodyhardtop     -4.116e+03  1.624e+03  -2.534 0.012181 *  \n#&gt; carbodyhatchback   -3.659e+03  1.295e+03  -2.826 0.005269 ** \n#&gt; carbodysedan       -2.464e+03  1.278e+03  -1.928 0.055483 .  \n#&gt; carbodywagon       -3.624e+03  1.389e+03  -2.609 0.009888 ** \n#&gt; carwidth            9.752e+02  1.661e+02   5.872 2.19e-08 ***\n#&gt; enginelocationrear  1.124e+04  2.112e+03   5.321 3.20e-07 ***\n#&gt; stroke             -2.422e+03  7.119e+02  -3.402 0.000833 ***\n#&gt; peakrpm             1.898e+00  5.516e-01   3.442 0.000726 ***\n#&gt; enginesize          1.311e+02  8.936e+00  14.676  &lt; 2e-16 ***\n#&gt; seat_heatingTRUE           NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2864 on 171 degrees of freedom\n#&gt; Multiple R-squared:  0.8803, Adjusted R-squared:  0.874 \n#&gt; F-statistic: 139.7 on 9 and 171 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficients obtained for the new variable “seat_heating” is NA. This can be explained by the fact that since all the values are True thereby this variable is linearly related to other variable leaving us no unique solution to the regression."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_probability.html#assumptions",
    "href": "content/01_journal/01_probability.html#assumptions",
    "title": "Probability Theory",
    "section": "",
    "text": "Assuming the same data as in the Probability tree, the related probabilities and their sum is calculate using the code below.\n\n#Constants Definition N&lt;-Not, I&lt;-|. so P(B|A) = P_BIA\nP_S&lt;-0.3\nP_NS&lt;-1-P_S\nP_TIS&lt;-0.2\nP_NTIS&lt;-1-P_TIS\nP_TINS&lt;-0.6\nP_NTINS&lt;-1-P_TINS\n\n#Calculation\nP1&lt;-P_S*P_TIS\nP2&lt;-P_S*P_NTIS\nP3&lt;-P_NS*P_TINS\nP4&lt;-P_NS*P_NTINS\nSum_of_P&lt;-P1+P2+P3+P4"
  },
  {
    "objectID": "content/01_journal/01_probability.html#results",
    "href": "content/01_journal/01_probability.html#results",
    "title": "Probability Theory",
    "section": "",
    "text": "P(T ⋂ S ) = 0.06\nP(T ⋂ !S ) = 0.24\nP(!T ⋂ S ) = 0.42\nP(!T ⋂ !S ) = 0.28\nSum of all = 1"
  }
]