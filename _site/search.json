[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nexpected value\nvariance\nstandard deviation\n\n\n\nlibrary(dplyr)\n#Getting Ages and their Prob\n\nrandom_vars &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/random_vars.rds\")\n\nunique_ages&lt;-unique(random_vars[\"age\"]) %&gt;% as.matrix()\nordered_ages&lt;-unique_ages[order(unique_ages, decreasing = FALSE)]\nprob_ages&lt;-prop.table(table(random_vars$age)) %&gt;% unclass()\n\n#Getting Income and their Prob\nunique_inc&lt;-unique(random_vars[\"income\"]) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(random_vars$income)) %&gt;% unclass()\n\nE_ages&lt;-sum(ordered_ages*prob_ages)\nE_inc&lt;-sum(ordered_inc*prob_inc)\n\n#Getting Variance\nVar_ages&lt;-var(random_vars$age)\nVar_inc&lt;-var(random_vars$income)\n\nSd_ages&lt;-sd(random_vars$age)\nSd_inc&lt;-sd(random_vars$income)\n\n\n\n#&gt; [1] \"E(Ages): 33.471\"\n\n\n#&gt; [1] \"E(Income): 3510.731\"\n\n\n#&gt; [1] \"Var(Ages): 340.607766766767\"\n\n\n#&gt; [1] \"Var(Income): 8625645.84448348\"\n\n\n#&gt; [1] \"SD(Ages): 18.4555619466536\"\n\n\n#&gt; [1] \"SD(Income): 2936.94498492626\"\n\n\n\nExplain, if it makes sense to compare the standard deviations.\n\nIt doesn’t make sense to compare the standard deviation. Both have different ranges of values in the population therefor comparisons of just the SD gives us no meaningful information.\n\nThen, examine the relationship between both variables and compute:\n\ncovariance\ncorrelation\n\n\n\nlibrary(dplyr)\n\ncov_data&lt;-cov(random_vars$age,random_vars$income)\ncor_data&lt;-cor(random_vars$age,random_vars$income)\n\n\n\n#&gt; [1] \"Covariance: 29700.1468458458\"\n\n\n#&gt; [1] \"Correlation: 0.547943162326477\"\n\n\n\nWhat measure is easier to interpret? Please discuss your interpretation.\n\nCorrelation is easier to interpret. Due to the normalization of correlation between -1 to 1 on can precisely determine how strong/weak the relation where has due to the boundless nature of covariance one can only say that the relation between the variables is strong/weak but not its extent.\n\nlibrary(dplyr)\n#Set 1\nset1&lt;-random_vars$income[random_vars$age&lt;=18]\n\nunique_inc&lt;-unique(set1) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set1)) %&gt;% unclass()\n\nE_set1&lt;-sum(ordered_inc*prob_inc)\n\n#Set 2\nset2&lt;-random_vars$income[random_vars$age&gt;=18 & random_vars$age&lt;65]\n\nunique_inc&lt;-unique(set2) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set2)) %&gt;% unclass()\n\nE_set2&lt;-sum(ordered_inc*prob_inc)\n\n#Set3\nset3&lt;-random_vars$income[random_vars$age&gt;=65]\n\nunique_inc&lt;-unique(set3) %&gt;% as.matrix()\nordered_inc&lt;-unique_inc[order(unique_inc, decreasing = FALSE)]\nprob_inc&lt;-prop.table(table(set3)) %&gt;% unclass()\n\nE_set3&lt;-sum(ordered_inc*prob_inc)\n\n\n\n#&gt; [1] \"E[income|age &lt;= 18]: 389.607438016529\"\n\n\n#&gt; [1] \"E[income|age ∈ [18,65)]: 4685.73426573427\"\n\n\n#&gt; [1] \"E[income|age &gt;= 65]: 1777.23728813559\""
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Check the relationships between the variables and draw a DAG as you understand the relations.\n\n\n#Check Correlation\ndf &lt;- readRDS(\"~/GitHub/cdsba-msandipan/Causal_Data_Science_Data/membership.rds\")\ndf %&gt;%\n     cor() %&gt;% \n     round(2) %&gt;% \n     Matrix::tril()\n\n#&gt; 5 x 5 Matrix of class \"dtrMatrix\"\n#&gt;                age  sex pre_avg_purch card avg_purch\n#&gt; age           1.00    .             .    .         .\n#&gt; sex           0.01 1.00             .    .         .\n#&gt; pre_avg_purch 0.52 0.00          1.00    .         .\n#&gt; card          0.11 0.01          0.19 1.00         .\n#&gt; avg_purch     0.45 0.00          0.86 0.38      1.00\n\n#DAG Graph\ndag_model &lt;- dagify(avg_purch~card,\n                    pre_avg_purch~age, \n                    card~age,\n                    card~pre_avg_purch,\n                    avg_purch~age,\n                    avg_purch~pre_avg_purch,\n                    labels = c(\n                    \"card\" = \"Card\",\n                    \"avg_purch\" = \"Avg_purch\",\n                    \"age\"=\"age\",\n                    \"pre_avg_purch\" = \"Pre_Avg_purch\",\n                    \"sex\"=\"sex\"),\n                    exposure = \"card\",\n                    outcome = \"avg_purch\")\nggdag(dag_model, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\n\nCompute a naive estimate of the average treatment effect.\n\n\nlm_naive&lt;-lm(avg_purch~card,data=df)\nsummary(lm_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nUse the following matching methods to obtain more precise estimates:\n\n(Coarsened) Exact Matching.\n\n\ncem = matchit(card~age+pre_avg_purch,\n              data=df,\n              method='cem',\n              estimant='ATE')\ndf_cem&lt;-match.data(cem)\nlm_cem&lt;-lm(avg_purch~card,data=df_cem,weights=weights)\nsummary(lm_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.152  -24.162   -5.557   15.382  189.466 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  75.5246     0.3952   191.1   &lt;2e-16 ***\n#&gt; card         15.2711     0.6084    25.1   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 29.97 on 9949 degrees of freedom\n#&gt; Multiple R-squared:  0.05955,    Adjusted R-squared:  0.05946 \n#&gt; F-statistic:   630 on 1 and 9949 DF,  p-value: &lt; 2.2e-16\n\n\n\nNearest-Neighbor Matching\n\n\nnn = matchit(card~age+pre_avg_purch,\n              data=df,\n              method=\"nearest\",\n              distance = \"mahalanobis\",\n              replace = T )\ndf_nn&lt;-match.data(nn)\nlm_nn&lt;-lm(avg_purch~card,data=df_nn,weights=weights)\nsummary(lm_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -21.159   -1.517   18.610  181.319 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.2067     0.5844  130.40   &lt;2e-16 ***\n#&gt; card         14.9524     0.7479   19.99   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.37 on 6930 degrees of freedom\n#&gt; Multiple R-squared:  0.05453,    Adjusted R-squared:  0.05439 \n#&gt; F-statistic: 399.7 on 1 and 6930 DF,  p-value: &lt; 2.2e-16\n\n\n\nInverse Probability Weighting.\n\n\nmodel_prop = glm(card~age+pre_avg_purch,\n              data=df,\n              family = binomial(link = \"logit\") )\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\nlm_ipw&lt;-lm(avg_purch~card,data=df_ipw,weights=ipw)\nsummary(lm_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -203.886  -29.009   -0.273   28.782  215.682 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2627     0.4320  162.65   &lt;2e-16 ***\n#&gt; card         14.9548     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.2 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05654,    Adjusted R-squared:  0.05645 \n#&gt; F-statistic: 599.2 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nThink about example from previous chapter (parking spots) and draw the DAG.\n\nWe take the variables as follows * X is the Parking spots * Y is the Sales * Z is the Location\nWe make the folloing assumption for the dependancies, * X and Z: Dependant, Location affects the size of the parking spots * Z and Y: Dependant, Location affects the Sales * X and Y, conditional on Z: Dependant as the size of the parking lot affects sales in a fixed location\n\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n# Chain\nchain &lt;- dagify(\n    X ~ Z,\n    Y ~ Z,\n    Y ~ X,\n    coords = list(x = c(Y = 3, Z = 2, X = 1),\n                  y = c(Y = 0, Z = 1, X = 0)),\n    labels = list(X = \"Parking Spots\",\n                Y = \"Sales\", \n                Z = \"Location\")\n)\n\n# Plot DAG\nggdag(chain) +\n    geom_dag_point(color = \"red\") +\n    geom_dag_text(color = \"white\") +\n    geom_dag_edges(edge_color = \"black\")+\n    geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\nIn the data, you find three variables: satisfaction, follow_ups and subscription. Perform the following steps:\n\nregress satisfaction on follow_ups\nregress satisfaction on follow_ups and account for subscription\n\n\n\ndf &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/customer_sat.rds\")\n\n# Not Conditioned\nlm_1&lt;-lm(satisfaction ~ follow_ups,data=df)\n\n#Plot\nlm_not_cond &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction ))+\n  geom_point()+\n  stat_smooth(method = \"lm\", se = F)\n\n\n#Conditioned\nlm_2&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Elite\", ])\nlm_3&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Premium\", ])\nlm_4&lt;-lm(satisfaction ~ follow_ups,data=df[df$subscription == \"Premium+\", ])\n#Plot\nlm_cond &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction,color= subscription ))+ geom_point()+stat_smooth(method = \"lm\", se = F)\n\n#Printing\nprint(\"Not conditioned coefficients:\")\n\n#&gt; [1] \"Not conditioned coefficients:\"\n\nlm_1$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   78.886047   -3.309302\n\nprint(\"Conditioned coefficients accounting for Elite:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Elite:\"\n\nlm_2$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   31.307692    1.653846\n\nprint(\"Conditioned coefficients accounting for Premium:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Premium:\"\n\nlm_3$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;   70.076923    3.076923\n\nprint(\"Conditioned coefficients accounting for Premium+:\")\n\n#&gt; [1] \"Conditioned coefficients accounting for Premium+:\"\n\nlm_4$coefficients\n\n#&gt; (Intercept)  follow_ups \n#&gt;       47.95        1.75\n\nlm_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlm_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nCheck whether the covariates are balanced across the groups. Use a plot to show it.\n\nThe covariates are the mobile device users and previous visits of users.\n\ndf &lt;- readRDS(\"~/GitHub/cdsba-msandipan/Causal_Data_Science_Data/abtest_online.rds\")\n\ncompare_visits &lt;-  \n    ggplot(df, \n           aes(x = chatbot, \n               y = previous_visit, \n               )) +\n    stat_summary(geom = \"errorbar\", \n                 width = .5,\n                 fun.data = \"mean_se\", \n                 fun.args = list(mult=1.96),\n                 show.legend = F)\n  \ndf$mobile_device_int&lt;-as.integer(as.logical(df$mobile_device))\n\ncompare_devices &lt;-  \n    ggplot(df, \n           aes(x = chatbot, \n               y = mobile_device_int, \n               )) +\n    stat_summary(geom = \"errorbar\", \n                 width = .5,\n                 fun.data = \"mean_se\", \n                 fun.args = list(mult=1.96),\n                 show.legend = F)\n  \ncompare_visits\n\n\n\n\n\n\n\ncompare_devices\n\n\n\n\n\n\n\n\n\nRun a regression to find the effect of chatbot on sales. Both Purchase and Purchase amount show a negative correlation with the usage of chatbots\n\n\nlm_purchase &lt;- lm(purchase ~ chatbot, data = df)\nlm_purchase_amt &lt;- lm(purchase_amount ~ chatbot, data = df)\n\nsummary(lm_purchase)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.4960 -0.3249 -0.2679  0.5040  0.7321 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.49597    0.02122  23.376  &lt; 2e-16 ***\n#&gt; chatbotTRUE -0.22811    0.02989  -7.633 5.36e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4725 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.05516,    Adjusted R-squared:  0.05421 \n#&gt; F-statistic: 58.26 on 1 and 998 DF,  p-value: 5.36e-14\n\nsummary(lm_purchase_amt)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\n\nFind subgroup-specific effects by including an interaction. Compute a CATE for one exemplary group. A subgroup could be for example mobile users.\n\n\nlm_mobile &lt;- lm(purchase_amount ~ chatbot*mobile_device, data = df)\nsummary(lm_mobile)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n\n\nIt’s not only of interest how much customers buy but also if the buy at all. Then, the dependent variable is binary (either 0 or 1) instead of continuous and the model of choice is the logistic regression. Use the outcome variable purchase and run a logistic regression. The coefficients are not as easily interpretable as before. Look it up and interpret the coefficient for chatbot.\n\nDue to usage of Chatbot being a categorial variable, the coefficient of Chatbot False is 0.49597 and the coefficient for Chatbot True is 0.26786.\n\nlm_purchase &lt;- lm(purchase ~ chatbot, data = df)\nsummary(lm_purchase)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.4960 -0.3249 -0.2679  0.5040  0.7321 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.49597    0.02122  23.376  &lt; 2e-16 ***\n#&gt; chatbotTRUE -0.22811    0.02989  -7.633 5.36e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4725 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.05516,    Adjusted R-squared:  0.05421 \n#&gt; F-statistic: 58.26 on 1 and 998 DF,  p-value: 5.36e-14"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Manually computing the mean satisfaction for treated and control hospitals before and after the treatment\n\n\n#Check Correlation\ndf &lt;- readRDS(\"~/GitHub/cdsba-msandipan/Causal_Data_Science_Data/hospdd.rds\")\n\ndf_rep&lt;-df\nhosp&lt;-unique(df$hospital)\ndf_rep$hospital&lt;-replace(df$hospital,df$hospital %in% sample(hosp,18),0)\ndf_rep$hospital&lt;-replace(df_rep$hospital,df_rep$hospital!=0,1)\n\nbefore_control &lt;- df_rep %&gt;%\n  filter(procedure == 0,hospital==0) %&gt;%pull(satis)\nbefore_treatment &lt;- df_rep %&gt;%\n  filter(procedure == 0,hospital==1) %&gt;%pull(satis)\n\nafter_control &lt;- df_rep %&gt;%\n  filter(procedure == 1,hospital==0) %&gt;%pull(satis)\nafter_treatment &lt;- df_rep %&gt;%\n  filter(procedure == 1,hospital==1) %&gt;%pull(satis)\n\ndiff_control &lt;- mean(before_control) - mean(after_control)\ndiff_treatment &lt;- mean(before_treatment) - mean(after_treatment)\n\ndiff_diff&lt;-diff_treatment-diff_control\n\n\nUsing a linear regression to compute the estimate. Also, include group and time fixed effects in the regression, i.e. one regressor for each month and one regressor for each hospital:\n\n\nlm_1&lt;-lm(satis~procedure*hospital,data=df_rep)\nsummary(lm_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * hospital, data = df_rep)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1937 -0.6604 -0.0896  0.5611  5.4760 \n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)         3.46615    0.02017 171.830  &lt; 2e-16 ***\n#&gt; procedure           0.77078    0.04702  16.393  &lt; 2e-16 ***\n#&gt; hospital           -0.07160    0.02620  -2.733  0.00629 ** \n#&gt; procedure:hospital  0.26607    0.05883   4.523  6.2e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9833 on 7364 degrees of freedom\n#&gt; Multiple R-squared:  0.133,  Adjusted R-squared:  0.1326 \n#&gt; F-statistic: 376.4 on 3 and 7364 DF,  p-value: &lt; 2.2e-16\n\nlm_2&lt;-lm(satis~procedure*(hospital+month),data=df_rep)\nsummary(lm_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * (hospital + month), data = df_rep)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.2055 -0.6605 -0.0915  0.5628  5.4878 \n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)         3.511552   0.028537 123.052  &lt; 2e-16 ***\n#&gt; procedure           0.682065   0.133727   5.100 3.47e-07 ***\n#&gt; hospital           -0.073260   0.026203  -2.796  0.00519 ** \n#&gt; month              -0.014180   0.006305  -2.249  0.02455 *  \n#&gt; procedure:hospital  0.267723   0.058820   4.552 5.41e-06 ***\n#&gt; procedure:month     0.022054   0.023332   0.945  0.34458    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9831 on 7362 degrees of freedom\n#&gt; Multiple R-squared:  0.1336, Adjusted R-squared:  0.133 \n#&gt; F-statistic:   227 on 5 and 7362 DF,  p-value: &lt; 2.2e-16\n\nlm_3&lt;-lm(satis~procedure*(as.factor(hospital)+as.factor(month)),data=df_rep)\nsummary(lm_3)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * (as.factor(hospital) + as.factor(month)), \n#&gt;     data = df_rep)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.2073 -0.6623 -0.0954  0.5605  5.4983 \n#&gt; \n#&gt; Coefficients: (3 not defined because of singularities)\n#&gt;                                 Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                     3.489254   0.027861 125.236  &lt; 2e-16 ***\n#&gt; procedure                       0.842756   0.075630  11.143  &lt; 2e-16 ***\n#&gt; as.factor(hospital)1           -0.073711   0.026213  -2.812  0.00494 ** \n#&gt; as.factor(month)2              -0.009608   0.039684  -0.242  0.80871    \n#&gt; as.factor(month)3               0.021969   0.039684   0.554  0.57988    \n#&gt; as.factor(month)4              -0.042392   0.048197  -0.880  0.37913    \n#&gt; as.factor(month)5              -0.082292   0.048197  -1.707  0.08779 .  \n#&gt; as.factor(month)6              -0.052056   0.048197  -1.080  0.28014    \n#&gt; as.factor(month)7              -0.081521   0.048197  -1.691  0.09080 .  \n#&gt; procedure:as.factor(hospital)1  0.268174   0.058838   4.558 5.25e-06 ***\n#&gt; procedure:as.factor(month)2           NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)3           NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)4    -0.075073   0.092972  -0.807  0.41942    \n#&gt; procedure:as.factor(month)5     0.006161   0.092972   0.066  0.94716    \n#&gt; procedure:as.factor(month)6    -0.053165   0.092972  -0.572  0.56745    \n#&gt; procedure:as.factor(month)7           NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9833 on 7355 degrees of freedom\n#&gt; Multiple R-squared:  0.1339, Adjusted R-squared:  0.1325 \n#&gt; F-statistic: 94.77 on 12 and 7355 DF,  p-value: &lt; 2.2e-16\n\n\nInclusion of the second method would be beneficial as the inclusion of the third method leads to 3 coefficients not defined because of singularities."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assuming the same data as in the Probability tree, the related probabilities and their sum is calculate using the code below.\n\n#Constants Definition N&lt;-Not, I&lt;-|. so P(B|A) = P_BIA\nP_S&lt;-0.3\nP_NS&lt;-1-P_S\nP_TIS&lt;-0.2\nP_NTIS&lt;-1-P_TIS\nP_TINS&lt;-0.6\nP_NTINS&lt;-1-P_TINS\n\n#Calculation\nP1&lt;-P_S*P_TIS\nP2&lt;-P_S*P_NTIS\nP3&lt;-P_NS*P_TINS\nP4&lt;-P_NS*P_NTINS\nSum_of_P&lt;-P1+P2+P3+P4\n\n\n\n\nP(T ⋂ S ) = 0.06\nP(T ⋂ !S ) = 0.24\nP(!T ⋂ S ) = 0.42\nP(!T ⋂ !S ) = 0.28\nSum of all = 1"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\n\n\n\nRead the data and check the dimensions. How many rows and how many columns does the data have?\n\n181 rows and 82 columns\n\nUse appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\nstr() gives us a quick overview of the different datatypes. typeof() gives us a better view of the type of integer data present.\n\ndf &lt;- readRDS(\"C:/Users/Sandipan Mukherjee/Documents/GitHub/cdsba-msandipan/Causal_Data_Science_Data/car_prices.rds\")\n\nstr(df)\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\ntypeof(df$wheelbase)\n\n#&gt; [1] \"double\"\n\n\n\nRun a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\nRelevant Factors based on p-value: * carbody * carwidth * enginelocation * stroke\n* peakrpm * enginesize\nThese factors were chosen based on their p-values indicating that the alternative hypothesis was true instead of the null hypothesis stating that there is a correlation between the variables in question.\n\nlm_imp &lt;- lm(price ~ carbody+carwidth+enginelocation+stroke+peakrpm+enginesize, data = df)\nsummary(lm_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ carbody + carwidth + enginelocation + stroke + \n#&gt;     peakrpm + enginesize, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8973.1 -1644.5  -136.5  1265.1 15085.1 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.697e+04  1.035e+04  -6.472 9.85e-10 ***\n#&gt; carbodyhardtop     -4.116e+03  1.624e+03  -2.534 0.012181 *  \n#&gt; carbodyhatchback   -3.659e+03  1.295e+03  -2.826 0.005269 ** \n#&gt; carbodysedan       -2.464e+03  1.278e+03  -1.928 0.055483 .  \n#&gt; carbodywagon       -3.624e+03  1.389e+03  -2.609 0.009888 ** \n#&gt; carwidth            9.752e+02  1.661e+02   5.872 2.19e-08 ***\n#&gt; enginelocationrear  1.124e+04  2.112e+03   5.321 3.20e-07 ***\n#&gt; stroke             -2.422e+03  7.119e+02  -3.402 0.000833 ***\n#&gt; peakrpm             1.898e+00  5.516e-01   3.442 0.000726 ***\n#&gt; enginesize          1.311e+02  8.936e+00  14.676  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2864 on 171 degrees of freedom\n#&gt; Multiple R-squared:  0.8803, Adjusted R-squared:  0.874 \n#&gt; F-statistic: 139.7 on 9 and 171 DF,  p-value: &lt; 2.2e-16\n\nconfint(lm_imp, level = 0.95)\n\n#&gt;                            2.5 %        97.5 %\n#&gt; (Intercept)        -8.740074e+04 -46545.470512\n#&gt; carbodyhardtop     -7.321743e+03   -909.386711\n#&gt; carbodyhatchback   -6.215052e+03  -1103.700157\n#&gt; carbodysedan       -4.985517e+03     58.389405\n#&gt; carbodywagon       -6.366137e+03   -882.064242\n#&gt; carwidth            6.473973e+02   1303.044319\n#&gt; enginelocationrear  7.068573e+03  15406.356002\n#&gt; stroke             -3.827414e+03  -1016.802715\n#&gt; peakrpm             8.096928e-01      2.987167\n#&gt; enginesize          1.135026e+02    148.779568\n\n\n\nChoose one regressor and\n\nexplain what data type it is and what values it can take on\nwhat effect is has on the price and what changing the value would have as a result\nwhether its effect is statistically significant.\n\n\nWe can choose to look at the factor “Peak RPM”. This factor has integer values ranging from 4200 to 6600. We see that it has a positive effect on the price of the car. As the “Peak RPM” increases the price increases and indicated by the positive coefficient in the linear function. From the p-value 0.000726 we can infer that this factor is highly statistically significant for the inference of the car prices.\n\nAdd a variable seat_heating to the data and assign a value TRUE for all observations. You can use e.g. df %&gt;% mutate(new_variable = value). Assign it to a new object and run a regression. What coefficient do you get for the new variable seat_heating and how can you explain it?\n\n\nlibrary(dplyr)\n\n\ndf&lt;- df %&gt;% mutate(seat_heating = TRUE)\nlm_imp &lt;- lm(price ~ carbody+carwidth+enginelocation+stroke+peakrpm+enginesize+seat_heating, data = df)\nsummary(lm_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ carbody + carwidth + enginelocation + stroke + \n#&gt;     peakrpm + enginesize + seat_heating, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8973.1 -1644.5  -136.5  1265.1 15085.1 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        -6.697e+04  1.035e+04  -6.472 9.85e-10 ***\n#&gt; carbodyhardtop     -4.116e+03  1.624e+03  -2.534 0.012181 *  \n#&gt; carbodyhatchback   -3.659e+03  1.295e+03  -2.826 0.005269 ** \n#&gt; carbodysedan       -2.464e+03  1.278e+03  -1.928 0.055483 .  \n#&gt; carbodywagon       -3.624e+03  1.389e+03  -2.609 0.009888 ** \n#&gt; carwidth            9.752e+02  1.661e+02   5.872 2.19e-08 ***\n#&gt; enginelocationrear  1.124e+04  2.112e+03   5.321 3.20e-07 ***\n#&gt; stroke             -2.422e+03  7.119e+02  -3.402 0.000833 ***\n#&gt; peakrpm             1.898e+00  5.516e-01   3.442 0.000726 ***\n#&gt; enginesize          1.311e+02  8.936e+00  14.676  &lt; 2e-16 ***\n#&gt; seat_heatingTRUE           NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2864 on 171 degrees of freedom\n#&gt; Multiple R-squared:  0.8803, Adjusted R-squared:  0.874 \n#&gt; F-statistic: 139.7 on 9 and 171 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficients obtained for the new variable “seat_heating” is NA. This can be explained by the fact that since all the values are True thereby this variable is linearly related to other variable leaving us no unique solution to the regression."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_probability.html#assumptions",
    "href": "content/01_journal/01_probability.html#assumptions",
    "title": "Probability Theory",
    "section": "",
    "text": "Assuming the same data as in the Probability tree, the related probabilities and their sum is calculate using the code below.\n\n#Constants Definition N&lt;-Not, I&lt;-|. so P(B|A) = P_BIA\nP_S&lt;-0.3\nP_NS&lt;-1-P_S\nP_TIS&lt;-0.2\nP_NTIS&lt;-1-P_TIS\nP_TINS&lt;-0.6\nP_NTINS&lt;-1-P_TINS\n\n#Calculation\nP1&lt;-P_S*P_TIS\nP2&lt;-P_S*P_NTIS\nP3&lt;-P_NS*P_TINS\nP4&lt;-P_NS*P_NTINS\nSum_of_P&lt;-P1+P2+P3+P4"
  },
  {
    "objectID": "content/01_journal/01_probability.html#results",
    "href": "content/01_journal/01_probability.html#results",
    "title": "Probability Theory",
    "section": "",
    "text": "P(T ⋂ S ) = 0.06\nP(T ⋂ !S ) = 0.24\nP(!T ⋂ S ) = 0.42\nP(!T ⋂ !S ) = 0.28\nSum of all = 1"
  }
]